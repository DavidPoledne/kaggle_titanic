{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9ffb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data matipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56a926b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "# Imputation\n",
    "\n",
    "def knn_imputer(df):\n",
    "    #split predictor on numeric and categorical\n",
    "    numeric_predictors = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "    categorical_predictors = df.select_dtypes(include=\"object\")\n",
    "\n",
    "    #get columns\n",
    "    numeric_columns = numeric_predictors.columns.values\n",
    "    categorical_columns = categorical_predictors.columns.values\n",
    "\n",
    "    #imputation by mean / most frequent\n",
    "    numeric_predictors = KNNImputer(n_neighbors=5).fit_transform(numeric_predictors)\n",
    "\n",
    "    # predictor numpy.array to pandas.dataframe\n",
    "    numeric_predictors = pd.DataFrame(numeric_predictors, columns=numeric_columns)\n",
    "    categorical_predictors = df[categorical_columns]\n",
    "    df_imputed = pd.concat([numeric_predictors, categorical_predictors], axis=1)\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def knn_imputer_ind(df):\n",
    "    #split predictor on numeric and categorical\n",
    "    numeric_predictors = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "    categorical_predictors = df.select_dtypes(include=\"object\")\n",
    "\n",
    "\n",
    "    #get columns\n",
    "    numeric_columns = numeric_predictors.columns.values\n",
    "    categorical_columns = categorical_predictors.columns.values\n",
    "    \n",
    "    indicator = MissingIndicator(features=\"missing-only\")\n",
    "    missing_mask = indicator.fit_transform(numeric_predictors)\n",
    "\n",
    "    numeric_predictors_miss = numeric_predictors.isna().sum()\n",
    "    numeric_predictors_miss = numeric_predictors_miss[numeric_predictors_miss != 0].index.values\n",
    "\n",
    "    miss_list = []\n",
    "    for col in numeric_predictors_miss:\n",
    "        miss_list.append(f\"{col}_was_misssing\")\n",
    "\n",
    "\n",
    "    indicator_df = pd.DataFrame(missing_mask, columns=miss_list)\n",
    "\n",
    "    #imputation by mean\n",
    "    numeric_predictors = KNNImputer(n_neighbors=5).fit_transform(numeric_predictors)\n",
    "\n",
    "    # predictor numpy.array to pandas.dataframe\n",
    "    numeric_predictors = pd.DataFrame(numeric_predictors, columns=numeric_columns)\n",
    "    numeric_predictors = pd.concat([numeric_predictors, indicator_df], axis=1)\n",
    "    categorical_predictors = df[categorical_columns]\n",
    "    df_imputed = pd.concat([numeric_predictors, categorical_predictors], axis=1)\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# Convert categorical features into boolean\n",
    "\n",
    "def get_dummies_fun(df):\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    return df\n",
    "\n",
    "def label_encoder_fun(df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for predictor in df.columns:\n",
    "        if df[predictor].dtype == object:\n",
    "            df[predictor] = le.fit_transform(df[predictor])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Scaling\n",
    "\n",
    "def standardization(x_train, x_test):\n",
    "    columns = x_train.columns.values\n",
    "    index = x_train.index\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_train = pd.DataFrame(x_train, columns=columns, index=index)\n",
    "\n",
    "    columns = x_test.columns.values\n",
    "    index = x_test.index\n",
    "    scaler = StandardScaler()\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    x_test = pd.DataFrame(x_test, columns=columns, index=index)\n",
    "    return x_train, x_test\n",
    "\n",
    "def normalization(x_train, x_test):\n",
    "    columns = x_train.columns.values\n",
    "    index = x_train.index\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_train = pd.DataFrame(x_train, columns=columns, index=index)\n",
    "\n",
    "    columns = x_test.columns.values\n",
    "    index = x_test.index\n",
    "    scaler = MinMaxScaler()\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    x_test = pd.DataFrame(x_test, columns=columns, index=index)\n",
    "    return x_train, x_test\n",
    "\n",
    "def no_scaling_fun(x_train, x_test):\n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "# Features selection\n",
    "\n",
    "random_search__n_iter = 10\n",
    "def predictors_selector(x_train, y_train):\n",
    "    model = LogisticRegression(penalty=\"l1\", max_iter=500, solver=\"liblinear\",)\n",
    "    pipe = Pipeline([\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    model_params = {\n",
    "        \"C\": np.linspace(0.00001, 0.1)\n",
    "    }\n",
    "    rand_search = RandomizedSearchCV(model, model_params, n_iter=random_search__n_iter)\n",
    "    rand_search.fit(x_train, y_train)\n",
    "    best_params = rand_search.best_params_\n",
    "    best_c = best_params[\"C\"]\n",
    "    log_reg_model = LogisticRegression(penalty=\"l1\", C=best_c, max_iter=10000, solver=\"liblinear\")\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    coefs = log_reg_model.coef_\n",
    "    columns = x_train.columns.values\n",
    "    non_zero_mask = coefs != 0\n",
    "    selected_predictors = columns[non_zero_mask[0]]\n",
    "    x_train = x_train[selected_predictors]\n",
    "    return x_train\n",
    "\n",
    "def no_feature_selector(x_train, y_train):\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4d0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of all option function\n",
    "def comparison(imputer, convert_fun, scaler, feature_selector, hyperparameters_tuning, iterator):\n",
    "    # Import data\n",
    "    df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "    # Drop columns\n",
    "    df = df.drop(\"Name\", axis=1) # has no meaning \n",
    "    df = df.drop(\"Cabin\", axis=1) # high percentage of missing data\n",
    "    df = df.drop(\"Ticket\", axis=1) # lot of unique string data type\n",
    "    df = df.drop(\"PassengerId\", axis=1) # has no meaning\n",
    "\n",
    "    # Imputation\n",
    "    df = imputer(df)\n",
    "    \n",
    "    # Drop remaining rows with missing data\n",
    "    # there is only few missing data\n",
    "    df = df.dropna()\n",
    "\n",
    "    df = convert_fun(df)\n",
    "\n",
    "    # Split data on predictor and responsible feature\n",
    "    x = df.drop(\"Survived\", axis=1)\n",
    "    y = df.Survived\n",
    "\n",
    "    # Split data on training and testing data\n",
    "    # for choosing best model\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=8)\n",
    "\n",
    "    # Scaling\n",
    "    x_train, x_test = scaler(x_train, x_test)\n",
    "\n",
    "    # Initializing of list for comparing models, no scaling, no features selection, no missing values indicator\n",
    "    models_list = []\n",
    "\n",
    "    # Set scorring\n",
    "    scorring = \"accuracy\"\n",
    "\n",
    "    # Features selection\n",
    "    x_train = feature_selector(x_train, y_train)\n",
    "\n",
    "    # Models\n",
    "\n",
    "    # No hyperparameters tuning\n",
    "    if hyperparameters_tuning == False:\n",
    "\n",
    "        # K nearest neighbors classifier \n",
    "        # defaul setting\n",
    "        model = KNeighborsClassifier()\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 1,\n",
    "            \"model\": \"K Neighbors Classifier\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Logistic regression \n",
    "        # penalty: None\n",
    "        model = LogisticRegression(penalty=None, max_iter=500)\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 2,\n",
    "            \"model\": \"Logistic Regression, penalty = none\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Logistic regression, penalty = l1\n",
    "\n",
    "        model = LogisticRegression(penalty=\"l1\", max_iter=500, solver=\"liblinear\")\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 3,\n",
    "            \"model\": \"Logistic Regression, penalty = l1\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Logistic regression, penalty = l2\n",
    "\n",
    "        model = LogisticRegression(penalty=\"l2\", max_iter=500, solver=\"liblinear\")\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 4,\n",
    "            \"model\": \"Logistic Regression, penalty = l2\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Decision tree classifier \n",
    "\n",
    "        model = DecisionTreeClassifier()\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 5,\n",
    "            \"model\": \"Decision Tree Classifier\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Bagging \n",
    "\n",
    "        estimator = DecisionTreeClassifier()\n",
    "        model = BaggingClassifier(estimator=estimator, bootstrap=True)\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 6,\n",
    "            \"model\": \"Bagging\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Random forest Classifier \n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 7,\n",
    "            \"model\": \"Random Forest Classifier\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Gradient boosting classifier \n",
    "\n",
    "        model = GradientBoostingClassifier()\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 8,\n",
    "            \"model\": \"Gradient Boosting Classifier\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Adaboost 1\n",
    "\n",
    "        model = AdaBoostClassifier()\n",
    "        cv = cross_validate(model, x_train, y_train, cv=10, scoring=scorring)\n",
    "        cv = round(float(cv[\"test_score\"].mean()),5)\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 9,\n",
    "            \"model\": \"Adaboost Classifier\",\n",
    "            \"accuracy\": cv\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "\n",
    "    # With hyperparameters tuning\n",
    "    else:\n",
    "        # K nearest neighbors classifier \n",
    "        # Tuning:\n",
    "        # n_neighbors\n",
    "\n",
    "        # Cell parrameters:\n",
    "        max_n_neighbors = 30\n",
    "\n",
    "        # Model\n",
    "        model = KNeighborsClassifier()\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__n_neighbors\": range(1,max_n_neighbors)\n",
    "        }\n",
    "        grid_search = GridSearchCV(pipe, param,cv=10, scoring=scorring)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 1,\n",
    "            \"model\": \"K Neighbors Classifier\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Logistic regression\n",
    "        # penalty: l1\n",
    "        # Tuning:\n",
    "        # c\n",
    "\n",
    "        # Cell parrameters:\n",
    "        min_c = 0.00001\n",
    "        max_c = 0.1\n",
    "\n",
    "        # Model\n",
    "        model = LogisticRegression(penalty=\"l1\", max_iter=500, solver=\"liblinear\")\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__C\": np.linspace(min_c, max_c)\n",
    "        }\n",
    "        grid_search = GridSearchCV(pipe, param, cv=10, scoring=scorring)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 3,\n",
    "            \"model\": \"Logistic Regression, penalty = l1\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Logistic regression\n",
    "        # penalty: l2\n",
    "        # Tuning:\n",
    "        # c\n",
    "\n",
    "        # Cell parrameters:\n",
    "        min_c = 0.00001\n",
    "        max_c = 0.1\n",
    "\n",
    "        # Model\n",
    "        model = LogisticRegression(penalty=\"l2\", max_iter=500, solver=\"liblinear\")\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__C\": np.linspace(min_c, max_c)\n",
    "        }\n",
    "        grid_search = GridSearchCV(pipe, param, cv=10, scoring=scorring)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 4,\n",
    "            \"model\": \"Logistic Regression, penalty = l2\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Decision tree classifier\n",
    "\n",
    "        # Tuning:\n",
    "        # max depth\n",
    "        # max leaf nodes\n",
    "        # ccp\n",
    "\n",
    "        # Cell parrameters:\n",
    "        random_search__n_iter = 10\n",
    "\n",
    "        max_depth_search = 12\n",
    "        max_leaf_nodes_search = 100\n",
    "        min_ccp = 0.0001\n",
    "        max_ccp = 0.001\n",
    "\n",
    "        # Model\n",
    "        model = DecisionTreeClassifier()\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__max_depth\": range(1,max_depth_search),\n",
    "            \"model__max_leaf_nodes\": range(2, max_leaf_nodes_search),\n",
    "            \"model__ccp_alpha\": np.linspace(min_ccp, max_ccp)\n",
    "        }\n",
    "        grid_search = RandomizedSearchCV(pipe, param, cv=10, scoring=scorring, n_iter=random_search__n_iter)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 5,\n",
    "            \"model\": \"Decision Tree Classifier\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Bagging\n",
    "\n",
    "        # Tuning:\n",
    "        # n estimators\n",
    "        # max depth\n",
    "        # max leaf nodes\n",
    "        # ccp\n",
    "\n",
    "        # Cell parrameters:\n",
    "        random_search__n_iter = 10\n",
    "\n",
    "        max_n_estimators = 202\n",
    "        max_depth_search = 10\n",
    "        max_leaf_nodes_search = 100\n",
    "        min_ccp = 0.0001\n",
    "        max_ccp = 0.001\n",
    "\n",
    "        # Model\n",
    "        estimator = DecisionTreeClassifier()\n",
    "        model = BaggingClassifier(estimator=estimator, bootstrap=True)\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__n_estimators\": range(200,max_n_estimators),\n",
    "            \"model__estimator__max_depth\": range(3,max_depth_search),\n",
    "            \"model__estimator__max_leaf_nodes\": range(20, max_leaf_nodes_search),\n",
    "            \"model__estimator__ccp_alpha\": np.linspace(min_ccp, max_ccp)\n",
    "        }\n",
    "        grid_search = RandomizedSearchCV(pipe, param, cv=10, scoring=scorring, n_iter=random_search__n_iter)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 6,\n",
    "            \"model\": \"Bagging\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Random forest classifier\n",
    "\n",
    "        # Tuning:\n",
    "        # n estimatiors\n",
    "        # max depth\n",
    "        # max leaf nodes\n",
    "        # max features\n",
    "\n",
    "        # Cell parrameters:\n",
    "        random_search__n_iter = 10\n",
    "        max_n_estimators = 300\n",
    "        max_leaf_nodes_search = 100\n",
    "        max_depth_search = 100\n",
    "        max_features_search = 6\n",
    "\n",
    "        # Model\n",
    "        model = RandomForestClassifier()\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__n_estimators\": range(100, max_n_estimators),\n",
    "            \"model__max_depth\": range(4, max_depth_search),\n",
    "            \"model__max_leaf_nodes\": range(20, max_leaf_nodes_search),\n",
    "            \"model__max_features\": range(1,max_features_search)\n",
    "        }\n",
    "        grid_search = RandomizedSearchCV(pipe, param, cv=10, scoring=scorring, n_iter=random_search__n_iter)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 7,\n",
    "            \"model\": \"Random Forest Classifier\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Gradient boosting classifier\n",
    "\n",
    "        # Tuning:\n",
    "        # n estimatiors\n",
    "        # max depth\n",
    "        # max leaf nodes\n",
    "        # max features\n",
    "\n",
    "        # Cell parrameters:\n",
    "        random_search__n_iter = 10\n",
    "\n",
    "        max_n_estimators = 200\n",
    "        min_learning_rate = 0.001\n",
    "        max_learning_rate = 0.5\n",
    "        max_depth_search = 10\n",
    "\n",
    "        # Model\n",
    "        model = GradientBoostingClassifier()\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__n_estimators\": range(100, max_n_estimators),\n",
    "            \"model__learning_rate\": np.linspace(min_learning_rate, max_learning_rate),\n",
    "            \"model__max_depth\": range(1, max_depth_search)\n",
    "        }\n",
    "        grid_search = RandomizedSearchCV(pipe, param, cv=10, scoring=scorring, n_iter=random_search__n_iter)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 8,\n",
    "            \"model\": \"Gradient Boosting Classifier\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "        # Adaboost classifier\n",
    "\n",
    "        # Tuning:\n",
    "        # n estimatiors\n",
    "        # max depth\n",
    "        # max leaf nodes\n",
    "        # max features\n",
    "\n",
    "        # Cell parrameters:\n",
    "        random_search__n_iter = 10\n",
    "\n",
    "        max_n_estimators = 150\n",
    "        min_learning_rate = 0.001\n",
    "        max_learning_rate = 0.5\n",
    "\n",
    "        # Model\n",
    "        model = AdaBoostClassifier()\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        pipe = Pipeline([\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        param = {\n",
    "            \"model__n_estimators\": range(1, max_n_estimators),\n",
    "            \"model__learning_rate\": np.linspace(min_learning_rate, max_learning_rate),\n",
    "        }\n",
    "        grid_search = RandomizedSearchCV(pipe, param, cv=10, scoring=scorring, n_iter=random_search__n_iter)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Best accuracy\n",
    "        best_accuracy = round(float(grid_search.best_score_),5)\n",
    "\n",
    "        # Show results\n",
    "        print(f\"Best accuracy: {best_accuracy}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Add score to summary\n",
    "        model_params = {\n",
    "            \"id\": 9,\n",
    "            \"model\": \"Adaboost Classifier\",\n",
    "            \"accuracy\": best_accuracy\n",
    "        }\n",
    "        model_params = [model_params[\"id\"], model_params[\"model\"], model_params[\"accuracy\"]]\n",
    "        models_list.append(model_params)\n",
    "\n",
    "\n",
    "\n",
    "    # Compare models\n",
    "    columns = [\"id\", \"Model\", \"Accuracy\"]\n",
    "    data_index = pd.DataFrame(models_list, columns=columns)\n",
    "    summary = pd.DataFrame(models_list, columns=columns, index=data_index[\"id\"].values).sort_values(by=\"Accuracy\", ascending=False)\n",
    "    \n",
    "    # Set setting\n",
    "\n",
    "    # Setting\n",
    "    setting = {\n",
    "        \"Imputation\": f\"{imputer.__name__}\",\n",
    "        \"Categorical Features Convert\": f\"{convert_fun.__name__}\",\n",
    "        \"Scaling\": f\"{scaler.__name__}\",\n",
    "        \"Feature selection\": f\"{feature_selector.__name__}\",\n",
    "        \"Hyperparameters tuning\": f\"{hyperparameters_tuning}\"\n",
    "    }\n",
    "\n",
    "    df_list = []\n",
    "    for i in range(summary.shape[0]):\n",
    "        df_list.append(list(setting.values()))\n",
    "    df = pd.DataFrame(df_list, columns=setting.keys(), index=data_index[\"id\"].values)\n",
    "    summary = pd.concat([summary, df], axis=1)\n",
    "    summary.to_csv(f\"../output/models_comparison_{iterator}.csv\")\n",
    "    #iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.72281\n",
      "Best parameters: {'model__n_neighbors': 21}\n",
      "Best accuracy: 0.78617\n",
      "Best parameters: {'model__C': np.float64(0.0755126530612245)}\n",
      "Best accuracy: 0.79458\n",
      "Best parameters: {'model__C': np.float64(0.04898469387755103)}\n",
      "Best accuracy: 0.81716\n",
      "Best parameters: {'model__max_leaf_nodes': 63, 'model__max_depth': 6, 'model__ccp_alpha': np.float64(0.00046734693877551023)}\n"
     ]
    }
   ],
   "source": [
    "# Compare all options\n",
    "knn_imputer_list = [knn_imputer, knn_imputer_ind]\n",
    "convert_fun_list = [get_dummies_fun, label_encoder_fun]\n",
    "scaler_list = [no_scaling_fun, normalization, standardization]\n",
    "feature_selector_list = [predictors_selector, no_feature_selector]\n",
    "hyperparameter_tuning_list = [False, True]\n",
    "\n",
    "# Set iterator\n",
    "iterator = 1\n",
    "for knn_imputer in knn_imputer_list:\n",
    "    for convert_fun in convert_fun_list:\n",
    "        for scaler in scaler_list:\n",
    "            for feature_selector in feature_selector_list:\n",
    "                for hyperparameter_tuning in hyperparameter_tuning_list:\n",
    "                    comparison(knn_imputer, convert_fun, scaler, feature_selector, hyperparameter_tuning, iterator)\n",
    "                    # Update iterator\n",
    "                    iterator += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_house_prices)",
   "language": "python",
   "name": "venv_house_prices"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
